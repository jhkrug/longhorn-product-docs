= Install {longhorn-product-name} Using Rancher
:current-version: {page-component-version}

One benefit of installing {longhorn-product-name} through Rancher Apps & Marketplace is that Rancher provides authentication to the {longhorn-product-name} UI.

If there is a new version of {longhorn-product-name} available, you will see an `Upgrade Available` sign on the `Apps & Marketplace` screen. You can click `Upgrade` button to upgrade Longhorn Manager. See more about upgrade xref:upgrades/upgrades.adoc[here].

== Prerequisites

Each node in the Kubernetes cluster where {longhorn-product-name} is installed must fulfill xref:installation-setup/requirements.adoc[these requirements.]

The xref:longhorn-system/system-access/longhorn-cli.adoc[Longhorn Command Line Tool] can be used to check the {longhorn-product-name} environment for potential issues.

== Installation

[NOTE]
====
For Kubernetes < v1.25, if your cluster still enables Pod Security Policy admission controller, set `Other Settings > Pod Security Policy` to `true` to install `longhorn-psp` PodSecurityPolicy resource which allows privileged Longhorn pods to start.
====

If Rancher version is 2.5.9 or before, we recommend creating a new project for {longhorn-product-name}, for example, `Storage`.

. In Rancher, go to *Apps* > *Repositories*.
+
image::screenshots/install/rancher-2.6/navigation.png[Image]
+
. Click on the *Create* button.
+
. Choose *OCI Repository* in the radio buttons and enter the prime oci chart path, then click *Create*.
+
image::screenshots/install/rancher-2.6/oci-repository.png[Image]
+
The repository should be added successfully.
+
image::screenshots/install/rancher-2.6/repository-added.png[Image]
+
. In *Apps* > *Chart*, you can see `suse-storage` available for installation.
+
image::screenshots/install/rancher-2.6/suse-storage-installed.png[Image]

After {longhorn-product-name} is installed, you can access the {longhorn-product-name} UI by navigating to the *Longhorn* option from Rancher left panel.

== Access UI With Network Policy Enabled

Note that when the Network Policy is enabled, access to the UI from Rancher may be restricted.

Rancher interacts with the {longhorn-product-name} UI via a service called remotedialer, which facilitates connections between Rancher and the downstream clusters it manages. This service allows a user agent to access the cluster through an endpoint on the Rancher server. Remotedialer connects to the {longhorn-product-name} UI service by using the Kubernetes API Server as a proxy.

However, when the Network Policy is enabled, the Kubernetes API Server may be unable to reach pods on different nodes. This occurs because the Kubernetes API Server operates within the network namespace of host without a dedicated per-pod IP address. If you're using the Calico CNI plugin, any process in the network namespace of host (such as the API Server) connecting to a pod triggers Calico to encapsulate the packet in IPIP before forwarding it to the remote host. The tunnel address is chosen as the source to ensure the remote host knows to encapsulate the return packets correctly.

In other words, to allow the proxy to work with the Network Policy, the Tunnel IP of each node must be identified and explicitly permitted in the policy.

You can find the Tunnel IP by:

[,shell]
----
$ kubectl get nodes -oyaml | grep "Tunnel"
      projectcalico.org/IPv4VXLANTunnelAddr: 10.42.197.0
      projectcalico.org/IPv4VXLANTunnelAddr: 10.42.99.0
      projectcalico.org/IPv4VXLANTunnelAddr: 10.42.158.0
      projectcalico.org/IPv4VXLANTunnelAddr: 10.42.80.0
----

Next, permit traffic in the Network Policy using the Tunnel IP. You may need to update the Network Policy whenever new nodes are added to the cluster.

[,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: longhorn-ui-frontend
  namespace: longhorn-system
spec:
  podSelector:
    matchLabels:
      app: longhorn-ui
  policyTypes:
  - Ingress
  ingress:
  - from:
    - ipBlock:
        cidr: 10.42.197.0/32
    - ipBlock:
        cidr: 10.42.99.0/32
    - ipBlock:
        cidr: 10.42.158.0/32
    - ipBlock:
        cidr: 10.42.80.0/32
    ports:
      - port: 8000
        protocol: TCP
----

Another way to resolve the issue is by running the server nodes with `egress-selector-mode: cluster`. For more information, see https://documentation.suse.com/cloudnative/rke2/latest/en/reference/server_config.html#_critical_configuration_values[RKE2 Server Configuration Reference] and https://documentation.suse.com/cloudnative/k3s/latest/en/networking/basic-network-options.html#_control_plane_egress_selector_configuration[K3s Control-Plane Egress Selector configuration].
